{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nageshnnr/digital-metallurgy-lab/blob/main/Welcome_To_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================================================================================\n",
        "# data_preprocessing.py - Data cleaning and preprocessing utilities\n",
        "# =======================================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "class ScrapDataPreprocessor:\n",
        "    \"\"\"\n",
        "    Preprocessing utilities for galvanized steel scrap survey data\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.metal_columns = ['Aluminum', 'Copper', 'Zinc', 'Lead', 'Brass', 'Bronze']\n",
        "        self.product_types = [\n",
        "            'Galvanized Coils',\n",
        "            'Galvanized Plain Sheets',\n",
        "            'Galvanized Corrugated Sheets',\n",
        "            'Galvanized Pipes & Tubes',\n",
        "            'Galvanized Wire'\n",
        "        ]\n",
        "\n",
        "    def clean_survey_responses(self, raw_data: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Clean and validate survey response data\"\"\"\n",
        "\n",
        "        # Remove incomplete responses\n",
        "        cleaned_data = raw_data.dropna(subset=['Facility', 'Product'])\n",
        "\n",
        "        # Standardize facility names\n",
        "        cleaned_data['Facility'] = cleaned_data['Facility'].str.strip().str.title()\n",
        "\n",
        "        # Validate metal percentages sum to ~100%\n",
        "        metal_sum = cleaned_data[self.metal_columns].sum(axis=1)\n",
        "        valid_composition = (metal_sum >= 95) & (metal_sum <= 105)\n",
        "        cleaned_data = cleaned_data[valid_composition]\n",
        "\n",
        "        # Convert volume units to tonnes\n",
        "        if 'Volume_Unit' in cleaned_data.columns:\n",
        "            cleaned_data = self._standardize_volume_units(cleaned_data)\n",
        "\n",
        "        # Add calculated fields\n",
        "        cleaned_data = self._add_calculated_fields(cleaned_data)\n",
        "\n",
        "        return cleaned_data\n",
        "\n",
        "    def _standardize_volume_units(self, data: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Convert various volume units to tonnes\"\"\"\n",
        "        conversion_factors = {\n",
        "            'kg': 0.001,\n",
        "            'tons': 1.0,\n",
        "            'tonnes': 1.0,\n",
        "            'mt': 1.0,\n",
        "            'pounds': 0.000453592\n",
        "        }\n",
        "\n",
        "        for unit, factor in conversion_factors.items():\n",
        "            mask = data['Volume_Unit'].str.lower() == unit\n",
        "            data.loc[mask, 'Volume_Tonnes'] = data.loc[mask, 'Volume'] * factor\n",
        "\n",
        "        return data\n",
        "\n",
        "    def _add_calculated_fields(self, data: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Add derived metrics\"\"\"\n",
        "\n",
        "        # Calculate scrap rate if production data available\n",
        "        if 'Production_Volume' in data.columns:\n",
        "            data['Scrap_Rate_Percent'] = (data['Monthly_Scrap_Tonnes'] /\n",
        "                                        data['Production_Volume']) * 100\n",
        "\n",
        "        # Calculate metal recovery potential\n",
        "        for metal in ['Zinc', 'Aluminum', 'Copper']:\n",
        "            if f'{metal}_Percentage' in data.columns:\n",
        "                data[f'{metal}_Recovery_Tonnes'] = (\n",
        "                    data['Annual_Scrap_Tonnes'] * data[f'{metal}_Percentage'] / 100\n",
        "                )\n",
        "\n",
        "        # Add efficiency category\n",
        "        if 'Scrap_Rate_Percent' in data.columns:\n",
        "            data['Efficiency_Category'] = pd.cut(\n",
        "                data['Scrap_Rate_Percent'],\n",
        "                bins=[0, 2, 5, 10, float('inf')],\n",
        "                labels=['Excellent', 'Good', 'Average', 'Needs_Improvement']\n",
        "            )\n",
        "\n",
        "        return data\n",
        "\n",
        "    def validate_data_quality(self, data: pd.DataFrame) -> Dict:\n",
        "        \"\"\"Generate data quality report\"\"\"\n",
        "\n",
        "        quality_report = {\n",
        "            'total_records': len(data),\n",
        "            'missing_values': data.isnull().sum().to_dict(),\n",
        "            'duplicate_records': data.duplicated().sum(),\n",
        "            'data_types': data.dtypes.to_dict(),\n",
        "            'value_ranges': {}\n",
        "        }\n",
        "\n",
        "        # Check value ranges for key metrics\n",
        "        numeric_columns = data.select_dtypes(include=[np.number]).columns\n",
        "        for col in numeric_columns:\n",
        "            quality_report['value_ranges'][col] = {\n",
        "                'min': float(data[col].min()),\n",
        "                'max': float(data[col].max()),\n",
        "                'mean': float(data[col].mean()),\n",
        "                'outliers': self._detect_outliers(data[col])\n",
        "            }\n",
        "\n",
        "        return quality_report\n",
        "\n",
        "    def _detect_outliers(self, series: pd.Series) -> int:\n",
        "        \"\"\"Detect outliers using IQR method\"\"\"\n",
        "        Q1 = series.quantile(0.25)\n",
        "        Q3 = series.quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "        outliers = ((series < lower_bound) | (series > upper_bound)).sum()\n",
        "        return int(outliers)\n",
        "\n",
        "# =======================================================================================\n",
        "# visualization_utils.py - Custom visualization functions\n",
        "# =======================================================================================\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "class ScrapVisualizationUtils:\n",
        "    \"\"\"\n",
        "    Custom visualization utilities for scrap analytics\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.color_palette = px.colors.qualitative.Set3\n",
        "        self.steel_colors = {\n",
        "            'Zinc': '#71797E',\n",
        "            'Aluminum': '#A8A8A8',\n",
        "            'Copper': '#B87333',\n",
        "            'Lead': '#2F4F4F',\n",
        "            'Brass': '#DAA520',\n",
        "            'Bronze': '#CD7F32'\n",
        "        }\n",
        "\n",
        "    def create_scrap_flow_diagram(self, data: pd.DataFrame) -> go.Figure:\n",
        "        \"\"\"Create Sankey diagram showing material flow\"\"\"\n",
        "\n",
        "        # Prepare data for Sankey\n",
        "        products = data['Product'].unique()\n",
        "        regions = data['Region'].unique()\n",
        "\n",
        "        # Create nodes\n",
        "        source_nodes = list(products)\n",
        "        target_nodes = [f\"{region}_Region\" for region in regions]\n",
        "        all_nodes = source_nodes + target_nodes\n",
        "\n",
        "        # Create links\n",
        "        links = []\n",
        "        for i, product in enumerate(products):\n",
        "            for j, region in enumerate(regions):\n",
        "                volume = data[(data['Product'] == product) &\n",
        "                            (data['Region'] == region)]['Annual_Scrap_Tonnes'].sum()\n",
        "                if volume > 0:\n",
        "                    links.append({\n",
        "                        'source': i,\n",
        "                        'target': len(products) + j,\n",
        "                        'value': volume\n",
        "                    })\n",
        "\n",
        "        fig = go.Figure(data=[go.Sankey(\n",
        "            node=dict(\n",
        "                pad=15,\n",
        "                thickness=20,\n",
        "                line=dict(color=\"black\", width=0.5),\n",
        "                label=all_nodes,\n",
        "                color=\"blue\"\n",
        "            ),\n",
        "            link=dict(\n",
        "                source=[link['source'] for link in links],\n",
        "                target=[link['target'] for link in links],\n",
        "                value=[link['value'] for link in links]\n",
        "            )\n",
        "        )])\n",
        "\n",
        "        fig.update_layout(\n",
        "            title_text=\"Galvanized Steel Scrap Flow: Products to Regions\",\n",
        "            font_size=12,\n",
        "            height=600\n",
        "        )\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def create_composition_heatmap(self, data: pd.DataFrame) -> go.Figure:\n",
        "        \"\"\"Create heatmap of metal composition by product\"\"\"\n",
        "\n",
        "        metal_cols = ['Zinc_Percentage', 'Aluminum_Percentage', 'Copper_Percentage',\n",
        "                     'Lead_Percentage', 'Brass_Percentage', 'Bronze_Percentage']\n",
        "\n",
        "        composition_matrix = data.groupby('Product')[metal_cols].mean()\n",
        "\n",
        "        fig = go.Figure(data=go.Heatmap(\n",
        "            z=composition_matrix.values,\n",
        "            x=[col.replace('_Percentage', '') for col in metal_cols],\n",
        "            y=composition_matrix.index,\n",
        "            colorscale='RdYlBu_r',\n",
        "            text=composition_matrix.values.round(1),\n",
        "            texttemplate=\"%{text}%\",\n",
        "            textfont={\"size\": 10}\n",
        "        ))\n",
        "\n",
        "        fig.update_layout(\n",
        "            title='Metal Composition Heatmap by Product Type',\n",
        "            xaxis_title='Metal Type',\n",
        "            yaxis_title='Product Type',\n",
        "            height=400\n",
        "        )\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def create_optimization_scatter(self, data: pd.DataFrame) -> go.Figure:\n",
        "        \"\"\"Create scatter plot for optimization opportunities\"\"\"\n",
        "\n",
        "        fig = px.scatter(\n",
        "            data,\n",
        "            x='Production_Capacity',\n",
        "            y='Scrap_Rate_Percent',\n",
        "            size='Annual_Scrap_Tonnes',\n",
        "            color='Region',\n",
        "            hover_data=['Facility', 'Product'],\n",
        "            title='Scrap Rate vs Production Capacity - Optimization Opportunities'\n",
        "        )\n",
        "\n",
        "        # Add trend line\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=data['Production_Capacity'],\n",
        "            y=np.poly1d(np.polyfit(data['Production_Capacity'],\n",
        "                                 data['Scrap_Rate_Percent'], 1))(data['Production_Capacity']),\n",
        "            mode='lines',\n",
        "            name='Trend Line',\n",
        "            line=dict(color='red', dash='dash')\n",
        "        ))\n",
        "\n",
        "        fig.update_layout(\n",
        "            xaxis_title='Production Capacity (tonnes/month)',\n",
        "            yaxis_title='Scrap Rate (%)',\n",
        "            height=500\n",
        "        )\n",
        "\n",
        "        return fig\n",
        "\n",
        "# =======================================================================================\n",
        "# model_evaluation.py - ML model validation utilities\n",
        "# =======================================================================================\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class ModelEvaluator:\n",
        "    \"\"\"\n",
        "    Model evaluation and validation utilities\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.evaluation_metrics = {}\n",
        "\n",
        "    def evaluate_regression_model(self, model, X, y, cv_folds=5):\n",
        "        \"\"\"Comprehensive evaluation of regression model\"\"\"\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        # Train model\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Predictions\n",
        "        y_pred_train = model.predict(X_train)\n",
        "        y_pred_test = model.predict(X_test)\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics = {\n",
        "            'train_r2': r2_score(y_train, y_pred_train),\n",
        "            'test_r2': r2_score(y_test, y_pred_test),\n",
        "            'train_mae': mean_absolute_error(y_train, y_pred_train),\n",
        "            'test_mae': mean_absolute_error(y_test, y_pred_test),\n",
        "            'train_rmse': np.sqrt(mean_squared_error(y_train, y_pred_train)),\n",
        "            'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
        "        }\n",
        "\n",
        "        # Cross-validation\n",
        "        cv_scores = cross_val_score(model, X, y, cv=cv_folds, scoring='r2')\n",
        "        metrics['cv_mean'] = cv_scores.mean()\n",
        "        metrics['cv_std'] = cv_scores.std()\n",
        "\n",
        "        self.evaluation_metrics = metrics\n",
        "        return metrics\n",
        "\n",
        "    def plot_model_performance(self, model, X_test, y_test):\n",
        "        \"\"\"Create model performance visualization\"\"\"\n",
        "\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "        # Actual vs Predicted\n",
        "        ax1.scatter(y_test, y_pred, alpha=0.6)\n",
        "        ax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "        ax1.set_xlabel('Actual Values')\n",
        "        ax1.set_ylabel('Predicted Values')\n",
        "        ax1.set_title('Actual vs Predicted Values')\n",
        "\n",
        "        # Residuals\n",
        "        residuals = y_test - y_pred\n",
        "        ax2.scatter(y_pred, residuals, alpha=0.6)\n",
        "        ax2.axhline(y=0, color='r', linestyle='--')\n",
        "        ax2.set_xlabel('Predicted Values')\n",
        "        ax2.set_ylabel('Residuals')\n",
        "        ax2.set_title('Residual Plot')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig"
      ],
      "metadata": {
        "id": "UpWfLXAKnH3F"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d65154a5"
      },
      "source": [
        "# requirements.txt\n",
        "!pip install \\\n",
        "pandas>=1.5.0 \\\n",
        "numpy>=1.24.0 \\\n",
        "matplotlib>=3.6.0 \\\n",
        "seaborn>=0.12.0 \\\n",
        "plotly>=5.15.0 \\\n",
        "scikit-learn>=1.3.0 \\\n",
        "openpyxl>=3.1.0 \\\n",
        "jupyter>=1.0.0"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5d16c47",
        "outputId": "7576662d-988e-4acb-a2f0-91830e7ca10a"
      },
      "source": [
        "# Assuming your raw data is in a CSV file\n",
        "try:\n",
        "    raw_data = pd.read_csv('/path/to/your/data.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Data file not found. Please replace '/path/to/your/data.csv' with the correct path.\")\n",
        "    # You might want to exit or handle this error differently depending on your needs\n",
        "    raw_data = pd.DataFrame() # Create an empty DataFrame to avoid errors later\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the data file: {e}\")\n",
        "    raw_data = pd.DataFrame() # Create an empty DataFrame to avoid errors later\n",
        "\n",
        "\n",
        "# Instantiate the preprocessor\n",
        "preprocessor = ScrapDataPreprocessor()\n",
        "\n",
        "# Clean the data\n",
        "if not raw_data.empty:\n",
        "    cleaned_data = preprocessor.clean_survey_responses(raw_data.copy()) # Use a copy to avoid modifying the original DataFrame\n",
        "\n",
        "    # Display the first few rows of the cleaned data\n",
        "    display(cleaned_data.head())\n",
        "else:\n",
        "    print(\"Data loading failed. Cannot proceed with cleaning.\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Data file not found. Please replace '/path/to/your/data.csv' with the correct path.\n",
            "Data loading failed. Cannot proceed with cleaning.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}